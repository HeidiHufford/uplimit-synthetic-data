{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeidiHufford/uplimit-synthetic-data/blob/main/Project_1_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 1: getting started\n",
        "\n",
        "Let's get started with installing the dependencies."
      ],
      "metadata": {
        "id": "NNCoSL4aqtb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install \"distilabel[hf-inference-endpoints,openai,ollama] @ git+https://github.com/argilla-io/distilabel.git@develop\" \"model2vec\" \"accelerate\" \"transformers>=4.54.0\" \"semhash\" \"datasets<4.0.0\" \"numpy<2.0.0\" -U -q"
      ],
      "metadata": {
        "id": "d7lR692EraJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Warning\n",
        "\n",
        "Google Colab doesn't prompt you to restart the environment in order to use\n",
        "numpy smaller than 2.0.0 after a downgrade. You need to do that manually. So go to the dropdown next to \"Run all\" and restart the session. Similarly, if you experience a notebook being stuck, try to go for a hard reset by disconecting and deleting the runtime."
      ],
      "metadata": {
        "id": "zv8q6s_8-rCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with Hugging Face\n",
        "\n",
        "Let's first [get our token](https://huggingface.co/settings/tokens) and then log in."
      ],
      "metadata": {
        "id": "IMIyBmWEswVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "AUj2wG8BvNQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or use the Google Colab secrets integration."
      ],
      "metadata": {
        "id": "3jHQfk-gvUQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(token=(userdata.get('HF_TOKEN')))"
      ],
      "metadata": {
        "id": "1hVsNgcuvUjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Hugging Face datsets"
      ],
      "metadata": {
        "id": "-wzZR3Mau5Z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the [`fka/awesome-chatgpt-prompts`](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts) dataset. This dataset holds a pretty neat collection prompts to use for language models."
      ],
      "metadata": {
        "id": "xUj0yEmovRDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
        "ds"
      ],
      "metadata": {
        "id": "VB7DXs5Ws0MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"train\"].features"
      ],
      "metadata": {
        "id": "1a0Kn-PXws-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"train\"][0]"
      ],
      "metadata": {
        "id": "TvYQdrUT8kbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then do some cool operations."
      ],
      "metadata": {
        "id": "8nwFXGZ6tbOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_cool_things(row):\n",
        "    row[\"act_prompt\"] = row[\"act\"] + row[\"prompt\"]\n",
        "    return row\n",
        "\n",
        "ds = ds.map(do_cool_things)\n",
        "ds"
      ],
      "metadata": {
        "id": "J1SifUwpuOgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"train\"][\"act_prompt\"][0]"
      ],
      "metadata": {
        "id": "Kt-Wrr-BdUQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also do cool batch operations to make them quicker for example, when you need to run inference on the batches."
      ],
      "metadata": {
        "id": "WuJetSPI24K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_cool_things(batch):\n",
        "    row_act_prompts = []\n",
        "    for act, prompt in zip(batch[\"act\"], batch[\"prompt\"]):\n",
        "        row_act_prompts.append(act+prompt)\n",
        "    batch[\"act_prompt\"] = row_act_prompts\n",
        "    return batch\n",
        "\n",
        "ds = ds.map(do_cool_things, batched=True)\n",
        "ds"
      ],
      "metadata": {
        "id": "uH2TKcc22QNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"train\"][\"act_prompt\"][0]"
      ],
      "metadata": {
        "id": "X7_kr_t4dwau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLMs\n",
        "\n",
        "### Serverless HF LLMs\n",
        "\n",
        "We can then search a model on Hugging Face and start calling LLMs. Let's use the [`meta-llama/Llama-3.2-3B-Instruct`](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) and find and use the snippet from calling API endpoints.\n"
      ],
      "metadata": {
        "id": "ncESKAAF3IvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from huggingface_hub import get_token\n",
        "\n",
        "client = OpenAI(\n",
        "\tbase_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=get_token()\n",
        ")\n",
        "\n",
        "messages = [\n",
        "\t{\n",
        "\t\t\"role\": \"user\",\n",
        "\t\t\"content\": \"What is the capital of France?\"\n",
        "\t}\n",
        "]\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "\tmodel=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "\tmessages=messages,\n",
        "\tmax_tokens=500,\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "id": "3MbkvF4h3YaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can also check the model card and [choose a specific inference provider](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct?inference_api=true&inference_provider=auto&language=python&client=openai). In that case, you would change `meta-llama/Llama-3.2-3B-Instruct` for `meta-llama/Llama-3.2-3B-Instruct:<provider_name>`. Let's give it a try with Together AI!"
      ],
      "metadata": {
        "id": "rjKOwnS8etfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-3.2-1B-Instruct:auto\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the capital of France?\"\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "completion"
      ],
      "metadata": {
        "id": "MJK1nrzCes5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Serverless LLMs within distilabel\n",
        "\n",
        "We can then use a prompt from this dataset to call [LLMs with distilabel](https://davidberenstein1957.github.io/distilabel/latest/components-gallery/llms/). Let's see how we can use the [InferenceEndpointsLLM](https://davidberenstein1957.github.io/distilabel/latest/components-gallery/llms/inferenceendpointsllm/#dedicated-inference-endpoints-or-tgi)."
      ],
      "metadata": {
        "id": "0zMmmoB_rWln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.models import InferenceEndpointsLLM\n",
        "\n",
        "llm = InferenceEndpointsLLM(\n",
        "    model_id=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        ")\n",
        "\n",
        "llm.load()\n",
        "llm.generate_outputs(inputs=[[\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "]])"
      ],
      "metadata": {
        "id": "SQFsAnuO5FKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local LLMs within distilabel\n",
        "\n",
        "We can also use a local LLM using the `TransformersLLM` implementation. Although, the quality of generated examples is limited, this allows us to iterate on pipelines and projects, before spending many compute credits.\n",
        "Some good examples are the [SmolLM2](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9) and [LFM2](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38) models series."
      ],
      "metadata": {
        "id": "s8-rKaQqwFXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.models.llms import TransformersLLM\n",
        "import torch\n",
        "\n",
        "llm = TransformersLLM(\n",
        "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"bfloat16\"\n",
        ")\n",
        "\n",
        "llm.load()\n",
        "llm.generate_outputs(inputs=[[\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "]])"
      ],
      "metadata": {
        "id": "VIIUs6XTvuCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using prompt templates\n",
        "\n",
        "### Custom prompt templates\n",
        "\n",
        "Besides the built-in templates, we can also define custom templates within [the TextGeneration Task](hhttps://davidberenstein1957.github.io/distilabel/latest/components-gallery/tasks/textgeneration/), where you define a placeholder inside a prompts to inject rows in your dataset."
      ],
      "metadata": {
        "id": "N-fw8x7CTKqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.steps.tasks import TextGeneration\n",
        "from distilabel.models import TransformersLLM\n",
        "\n",
        "# initialise the LLM\n",
        "llm = TransformersLLM(\n",
        "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"bfloat16\"\n",
        ")\n",
        "\n",
        "# define you custom template\n",
        "system_prompt = \"You are a helpful assistant that is great a rewriting content.\"\n",
        "prompt_template = \"\"\"Rewrite the following instruction to make it more complex:\n",
        "\n",
        "{{instruction}}\n",
        "\n",
        "IMPORTANT. Only return the rewritten instruction and nothing else!\n",
        "Rewritten instruction:\n",
        "\"\"\"\n",
        "text_generation = TextGeneration(\n",
        "    name=\"exam_generation\",\n",
        "    system_prompt=system_prompt,\n",
        "    template=prompt_template,\n",
        "    llm=llm,\n",
        "    input_batch_size=8,\n",
        "    columns=[\"instruction\"]\n",
        ")\n",
        "text_generation.load()\n",
        "\n",
        "next(\n",
        "    text_generation.process([\n",
        "        {\"instruction\": \"What is the current capital of France?\"}\n",
        "    ]\n",
        "))"
      ],
      "metadata": {
        "id": "UW5CFisaUelO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Built-in prompt templates\n",
        "\n",
        "We can also use these LLMs along with the prompt templates. Prompt templates are called [tasks](https://davidberenstein1957.github.io/distilabel/latest/components-gallery/tasks/). We've already discussed the [EvolInstruct](https://davidberenstein1957.github.io/distilabel/latest/components-gallery/tasks/selfinstruct), [SelfInstruct](https://davidberenstein1957.github.io/distilabel/latest/components-gallery/tasks/selfinstruct/) and [Magpie](https://davidberenstein1957.github.io/distilabel/latest/components-gallery/tasks/magpie/) templates, let's try to use it now."
      ],
      "metadata": {
        "id": "4iI1E2dM76Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.steps.tasks import SelfInstruct\n",
        "from distilabel.models import TransformersLLM\n",
        "\n",
        "# initialise the LLM\n",
        "llm = TransformersLLM(\n",
        "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"bfloat16\"\n",
        ")\n",
        "\n",
        "# Consider this as a placeholder for your actual LLM.\n",
        "self_instruct = SelfInstruct(\n",
        "    llm=llm\n",
        ")\n",
        "self_instruct.load()\n",
        "\n",
        "next(\n",
        "    self_instruct.process([\n",
        "        {\"input\": \"The current capital of France is Paris.\"}\n",
        "    ]\n",
        "))"
      ],
      "metadata": {
        "id": "aK2YsCsy95eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the magpie template is slightly different and allows you to define some additional parameters during the initialisation of your LLM, like `tokenizer_id`, `magpie_pre_query_template` and `use_magpie_template`. A\n",
        "\n",
        "As we can see, the column of generated `instructions` is formatted as a list of strings. This means we can not directly use it in follow-up steps. To solve this this we've come up with formatting operations."
      ],
      "metadata": {
        "id": "8fx0zOqsTeWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using formatting operations\n",
        "\n",
        "### Built-in formatting using steps\n",
        "\n",
        "Besides prompt remplates, we also support [several formatting operations](https://davidberenstein1957.github.io/distilabel/latest/components-gallery/steps/) to format data and columns. Let's take a look at how to expand columns using the [ExpandColumns](https://davidberenstein1957.github.io/distilabel/latest/components-gallery/steps/expandcolumns/) Step.\n",
        "\n"
      ],
      "metadata": {
        "id": "8_IxbDCVqwwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.steps import ExpandColumns\n",
        "\n",
        "expand_columns = ExpandColumns(\n",
        "    columns=[\"generation\"],\n",
        ")\n",
        "\n",
        "next(\n",
        "    expand_columns.process(\n",
        "        [\n",
        "            {\n",
        "                \"instruction\": \"instruction 1\",\n",
        "                \"generation\": [\"generation 1\", \"generation 2\"]}\n",
        "        ],\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "OqAkiEkyrYLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you do not like this interface, we recommend taking your data outside of the distilabel flow and simply edit it with any other tool you prefer like Pandas or Hugging Face datasets."
      ],
      "metadata": {
        "id": "GM9NJXEgu-hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLMs in a Distilabel pipeline\n",
        "\n",
        "Normally, distilabel works with [pipelines](https://davidberenstein1957.github.io/distilabel/latest/sections/getting_started/quickstart/#define-a-custom-pipeline). We can use these to define a custom synthetic data flow. These pipelines are very useful for verifying the generations, caching the generations, serialising the pipeline, and reusing the pipeline when needed. Let's try to rewrite prompts from [a basic instruction dataset](https://huggingface.co/datasets/distilabel-internal-testing/instruction-dataset-mini?viewer_embed=true)."
      ],
      "metadata": {
        "id": "YJSqhOToAsGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami\n",
        "\n",
        "from distilabel.models import TransformersLLM\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps import LoadDataFromHub\n",
        "from distilabel.steps.tasks import EvolInstruct\n",
        "\n",
        "# the with Pipeline context window calls `load()` on components\n",
        "with Pipeline() as pipeline:\n",
        "    loader = LoadDataFromHub(\n",
        "        repo_id=\"distilabel-internal-testing/instruction-dataset-mini\",\n",
        "        split=\"test\",\n",
        "        num_examples=1\n",
        "    )\n",
        "    evol_instruct = EvolInstruct(\n",
        "        llm=TransformersLLM(\n",
        "            model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=\"bfloat16\"\n",
        "        ),\n",
        "        num_evolutions=1,\n",
        "        # ensure correct column mapping\n",
        "        input_mappings={\"instruction\": \"prompt\"},\n",
        "    )\n",
        "    loader.connect(evol_instruct)\n",
        "    # use connect to determine flow of data\n",
        "    # or use the use '>>' operator: `loader >> evol_instruct`\n",
        "pipeline"
      ],
      "metadata": {
        "id": "Jg2venxtnzbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distiset = pipeline.run(\n",
        "    parameters={\n",
        "        evol_instruct.name: {\n",
        "            \"llm\": {\n",
        "                \"generation_kwargs\":{\n",
        "                    \"temperature\": 0.7,\n",
        "                    \"max_new_tokens\": 1024\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "distiset"
      ],
      "metadata": {
        "id": "pUuKdTAd6tD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distiset[\"default\"][\"train\"][\"prompt\"], distiset[\"default\"][\"train\"][\"completion\"]"
      ],
      "metadata": {
        "id": "amA88JAm13_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload your dataset\n",
        "\n",
        "You can then push your dataset to Hugging Face and [create a nice dataset card](https://huggingface.co/datasets/uplimit/uplimit-synthetic-data-week-1-basic)."
      ],
      "metadata": {
        "id": "CmLB63gUJDkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distiset.push_to_hub(\"davidberenstein1957/example-dataset-distilabel\")"
      ],
      "metadata": {
        "id": "scruL856sNlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore your data\n",
        "\n",
        "There is [an integration with Nomic AI](https://huggingface.co/blog/MaxNomic/explore-any-hugging-face-dataset-with-nomic-atlas) that allows you to Explore, Curate and Vector Search Any Hugging Face Dataset with Nomic Atlas. Additionally, you could use something like [Argilla](https://huggingface.co/blog/argilla-ui-hub) for a more fine-grained analysis.\n",
        "\n",
        "\n",
        "Let's start with exploring the data in [Nomic AI](https://atlas.nomic.ai/data/davidmberenstein/distilabel-intel-orca-dpo-pairs/map/58350d76-78cf-4383-ad65-2d4f562dabcf)."
      ],
      "metadata": {
        "id": "Qz0ol315Eik8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deduplicate you data\n",
        "\n",
        "The [Dataset Tools organisation on Hugging Face](https://huggingface.co/collections/Dataset-Tools/models-for-dataset-curation-673c647d85be6398f9ba23d3) hold collections of tools and models to explore data or do feature engineering. For example, there is a really fast embedder which is based on [Model2Vec](https://github.com/MinishLab/model2vec) and that can be used to deduplicate data based on semantic overlab using [semhash](https://github.com/MinishLab/semhash/tree/main/semhash).\n"
      ],
      "metadata": {
        "id": "KsqC_YLTFSty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from semhash import SemHash\n",
        "\n",
        "\n",
        "ds = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
        "\n",
        "semhash = SemHash.from_records(records=ds[\"input\"])\n",
        "\n",
        "# Deduplicate the texts\n",
        "deduplicated_texts = semhash.self_deduplicate(threshold=0.8).deduplicated\n",
        "print(f\"Original dataset: {len(ds)}. Filtered dataset: {len(deduplicated_texts)}. Percentage left: {len(deduplicated_texts)/len(ds)}\")"
      ],
      "metadata": {
        "id": "Z_hm8f2iFszC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter data on quality\n",
        "\n",
        "Similarly, you can there are models and tools, to determine the quality of your texts. A model we could use is the [`HuggingFaceFW/fineweb-edu-classifier`](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier) model for educational quality. However,\n",
        "[Text Descriptives](https://github.com/HLasse/TextDescriptives) is another Python library you can explore for calculating a large variety of quality metrics from text."
      ],
      "metadata": {
        "id": "0SvjlSQ8FvOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "ds = load_dataset(\n",
        "    path=\"argilla/distilabel-intel-orca-dpo-pairs\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"HuggingFaceFW/fineweb-edu-classifier\"\n",
        ")\n",
        "\n",
        "quality_predictions = pipe(ds[\"chosen\"], truncation=True, verbose=True)\n",
        "\n",
        "\n",
        "quality_scores = [i[\"score\"] for i in quality_predictions]\n",
        "\n",
        "df = pd.DataFrame.from_dict(\n",
        "    {\n",
        "        \"text\": ds[\"chosen\"],\n",
        "        \"quality\": quality_scores\n",
        "    }\n",
        ")\n",
        "p_to_keep = 0.8\n",
        "min_score = 0.8\n",
        "df.sort_values(by=\"quality\", ascending=False, inplace=True)\n",
        "df = df.head(int(len(df)*p_to_keep))\n",
        "df = df[df[\"quality\"] > min_score]\n",
        "print(f\"Original dataset: {len(ds)}. Filtered dataset: {len(df)}. Percentage left: {len(df)/len(ds)}\")"
      ],
      "metadata": {
        "id": "DDNV8cFACbOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}